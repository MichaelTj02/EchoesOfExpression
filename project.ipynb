{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (1.66.3)\n",
      "Requirement already satisfied: pillow in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (11.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.13 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.1.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.13 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install diffusers transformers accelerate scipy safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.13 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# For API keys\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Open AI GPT-4\n",
    "import openai\n",
    "\n",
    "# Handwriting OCR\n",
    "import cv2\n",
    "import numpy as np\n",
    "import base64\n",
    "\n",
    "# For sentiment analysis\n",
    "import json\n",
    "import re\n",
    "\n",
    "# For stable diffusion\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# For canvas and drawing\n",
    "from PIL import Image, ImageEnhance, ImageDraw\n",
    "import random\n",
    "from IPython.display import display, clear_output\n",
    "import time # For live drawing\n",
    "from collections import deque\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Check if API key is loaded\n",
    "if not openai.api_key:\n",
    "    raise ValueError(\"API Key not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create client \n",
    "client = openai.OpenAI(api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key Loaded Successfully!\n",
      "ChatGPT Response: Hello! How can I assist you with your Jupyter Notebook or any other queries you might have today?\n"
     ]
    }
   ],
   "source": [
    "# Test API call with the updated function\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello from Jupyter Notebook!\"}]\n",
    ")\n",
    "\n",
    "print(\"API Key Loaded Successfully!\")\n",
    "print(\"ChatGPT Response:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Drawing Agents (Reactive Systems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, image, start_x, start_y, patch_size=5):\n",
    "        self.image = image.convert(\"RGBA\")  # Ensure image has alpha\n",
    "        self.patch_size = patch_size\n",
    "        self.origin_x = start_x\n",
    "        self.origin_y = start_y\n",
    "        self.visited = set()\n",
    "        self.queue = deque()\n",
    "\n",
    "        # Drawing mode determines shape of reveal\n",
    "        self.drawing_mode = random.choice([\n",
    "            \"center_out\", \n",
    "            \"top_down\", \n",
    "            \"left_to_right\", \n",
    "            \"spiral\", \n",
    "            \"organic_noise\"\n",
    "        ])\n",
    "\n",
    "        center_x = self.origin_x + self.image.width // 2\n",
    "        center_y = self.origin_y + self.image.height // 2\n",
    "\n",
    "        if self.drawing_mode == \"center_out\" or self.drawing_mode == \"spiral\":\n",
    "            self.queue.append((center_x, center_y))\n",
    "\n",
    "        elif self.drawing_mode == \"top_down\":\n",
    "            for x in range(self.origin_x, self.origin_x + self.image.width, self.patch_size):\n",
    "                self.queue.append((x, self.origin_y))\n",
    "\n",
    "        elif self.drawing_mode == \"left_to_right\":\n",
    "            for y in range(self.origin_y, self.origin_y + self.image.height, self.patch_size):\n",
    "                self.queue.append((self.origin_x, y))\n",
    "\n",
    "        elif self.drawing_mode == \"organic_noise\":\n",
    "            for _ in range(10):\n",
    "                rx = random.randint(self.origin_x, self.origin_x + self.image.width - self.patch_size)\n",
    "                ry = random.randint(self.origin_y, self.origin_y + self.image.height - self.patch_size)\n",
    "                self.queue.append((rx, ry))\n",
    "\n",
    "    def update(self, canvas):\n",
    "        if not self.queue:\n",
    "            return\n",
    "\n",
    "        x, y = self.queue.popleft()\n",
    "        key = (x, y)\n",
    "        if key in self.visited:\n",
    "            return\n",
    "\n",
    "        self.visited.add(key)\n",
    "\n",
    "        # Get image patch\n",
    "        patch = self.image.crop((\n",
    "            max(0, x - self.origin_x),\n",
    "            max(0, y - self.origin_y),\n",
    "            max(0, x - self.origin_x + self.patch_size),\n",
    "            max(0, y - self.origin_y + self.patch_size)\n",
    "        ))\n",
    "\n",
    "        # Fade alpha based on distance from center\n",
    "        center_x = self.origin_x + self.image.width // 2\n",
    "        center_y = self.origin_y + self.image.height // 2\n",
    "        dist = ((x - center_x) ** 2 + (y - center_y) ** 2) ** 0.5\n",
    "        max_dist = ((self.image.width // 2) ** 2 + (self.image.height // 2) ** 2) ** 0.5\n",
    "        fade = max(0.2, 1.0 - (dist / max_dist))  # Avoid full transparency\n",
    "\n",
    "        patch = patch.copy()\n",
    "        r, g, b, a = patch.split()\n",
    "        a = a.point(lambda p: int(p * fade))\n",
    "        patch.putalpha(a)\n",
    "\n",
    "        canvas.paste(patch, (x, y), patch)\n",
    "\n",
    "        # Base directions (4-neighbors)\n",
    "        directions = [\n",
    "            (self.patch_size, 0), (-self.patch_size, 0),\n",
    "            (0, self.patch_size), (0, -self.patch_size)\n",
    "        ]\n",
    "\n",
    "        # Modify expansion pattern\n",
    "        if self.drawing_mode == \"spiral\":\n",
    "            directions = sorted(directions, key=lambda d: random.random() + 0.3 * (d[0] + d[1]))\n",
    "\n",
    "        elif self.drawing_mode == \"top_down\":\n",
    "            directions = sorted(directions, key=lambda d: d[1])  # prioritize y (vertical)\n",
    "\n",
    "        elif self.drawing_mode == \"left_to_right\":\n",
    "            directions = sorted(directions, key=lambda d: d[0])  # prioritize x (horizontal)\n",
    "\n",
    "        elif self.drawing_mode == \"organic_noise\":\n",
    "            random.shuffle(directions)\n",
    "\n",
    "        # Expand to neighbors\n",
    "        for dx, dy in directions:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if (nx, ny) not in self.visited:\n",
    "                if self.origin_x <= nx < self.origin_x + self.image.width - self.patch_size and \\\n",
    "                   self.origin_y <= ny < self.origin_y + self.image.height - self.patch_size:\n",
    "                    self.queue.append((nx, ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Handwritten Text (OCR, Translation, and Emotion Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Processing methods\n",
    "\n",
    "# Preprocess handwriting image to make it more legible\n",
    "def preprocess_handwriting(image_path):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Gaussian Blur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Apply adaptive thresholding (Binarization)\n",
    "    processed = cv2.adaptiveThreshold(\n",
    "        blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2\n",
    "    )\n",
    "\n",
    "    # Save processed image\n",
    "    processed_path = \"processed_handwriting.jpg\"\n",
    "    cv2.imwrite(processed_path, processed)\n",
    "\n",
    "    return processed_path\n",
    "\n",
    "# Convert image to Base64\n",
    "def image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        return base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
    "    \n",
    "# Extract handwriting image\n",
    "def extract_text_from_handwriting(image_path):\n",
    "    base64_image = image_to_base64(image_path)  # Convert image to base64\n",
    "\n",
    "    try:\n",
    "        # Send request to OpenAI GPT-4o\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \n",
    "                            \"Extract the handwritten text from this image and detect its language.\"\n",
    "                            \"Return the result in JSON format: {\\\"text\\\": \\\"extracted text\\\", \\\"language\\\": \\\"detected language\\\"}.\"\n",
    "                            \"Do not add extra explanations, just return valid JSON.\"\n",
    "                        },\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=500\n",
    "        )\n",
    "\n",
    "        raw_response = response.choices[0].message.content.strip()\n",
    "        # print(f\"\\n Raw GPT Response: {raw_response}\")  # Debugging Output\n",
    "\n",
    "        json_match = re.search(r\"\\{.*\\}\", raw_response, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_text = json_match.group(0)  # Extract JSON-only content\n",
    "        else:\n",
    "            raise ValueError(\"Invalid JSON format received from GPT.\")\n",
    "\n",
    "        # Parse cleaned JSON response\n",
    "        text_data = json.loads(json_text)\n",
    "        extracted_text = text_data.get(\"text\", \"Unknown\")\n",
    "        detected_language = text_data.get(\"language\", \"Unknown\")\n",
    "\n",
    "        return extracted_text, detected_language\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI API Error: {e}\")\n",
    "        return None, \"Unknown\"\n",
    "    \n",
    "# Detect language & translate to English using GPT-4o\n",
    "def translate_to_english(text):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": f\"Detect the language of this text: '{text}'. If it's not English, translate it to English. If it's already in English, return the same text.\"}\n",
    "            ],\n",
    "            max_tokens=500\n",
    "        )\n",
    "\n",
    "        translated_text = response.choices[0].message.content.strip()\n",
    "        # print(f\"\\n Translated Text (English): {translated_text}\")  # Debugging output\n",
    "        return translated_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Translation Error: {e}\")\n",
    "        return text  # If translation fails, return original text\n",
    "    \n",
    "# Analyze emotion\n",
    "def analyze_emotion(text):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": f\"Analyze the following text: '{text}'. \"\n",
    "                 \"Return ONLY a JSON object with the following structure: \"\n",
    "                 \"{{\\\"emotion\\\": \\\"Happiness\\\", \\\"confidence\\\": 0.92, \\\"language\\\": \\\"English\\\"}}. \"\n",
    "                 \"The 'emotion' should be one of: Happiness, Sadness, Fear, Anger, Surprise, or Disgust. \"\n",
    "                 \"The 'confidence' should be a float between 0 and 1. \"\n",
    "                 \"The 'language' should be a single-word language name (e.g., English, Japanese, Chinese). \"\n",
    "                 \"No explanations, no extra text—return only a valid JSON output.\"}\n",
    "            ],\n",
    "            max_tokens=100\n",
    "        )\n",
    "\n",
    "        raw_response = response.choices[0].message.content.strip()\n",
    "        # print(f\"\\n Raw GPT Response: {raw_response}\")  # Debugging output\n",
    "\n",
    "        # Extract JSON using regex if there's extra text\n",
    "        json_match = re.search(r\"\\{.*\\}\", raw_response, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_text = json_match.group(0)  # Extracts the JSON part only\n",
    "        else:\n",
    "            raise ValueError(\"Invalid JSON format received from GPT.\")\n",
    "\n",
    "        # Parse JSON safely\n",
    "        text_analysis_data = json.loads(json_text)\n",
    "\n",
    "        # Extract emotion, confidence, and language\n",
    "        detected_emotion = text_analysis_data.get(\"emotion\", \"Unknown\")\n",
    "        confidence_score = text_analysis_data.get(\"confidence\", 0.0)\n",
    "        detected_language = text_analysis_data.get(\"language\", \"Unknown\")\n",
    "\n",
    "        return {\n",
    "            \"emotion\": detected_emotion,\n",
    "            \"confidence\": confidence_score,\n",
    "            \"language\": detected_language\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Text Analysis Error: {e}\")\n",
    "        return {\n",
    "            \"emotion\": \"Unknown\",\n",
    "            \"confidence\": 0.0,\n",
    "            \"language\": \"Unknown\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_handwritten_image(processed_image):\n",
    "    global extracted_data  # Declare global variable\n",
    "    \n",
    "    extracted_text, detected_language = extract_text_from_handwriting(processed_image)\n",
    "\n",
    "    if extracted_text:\n",
    "        translated_text = translate_to_english(extracted_text) if detected_language.lower() != \"english\" else extracted_text\n",
    "        text_analysis_data = analyze_emotion(translated_text)\n",
    "\n",
    "        detected_emotion = text_analysis_data[\"emotion\"]\n",
    "        confidence_score = text_analysis_data[\"confidence\"]\n",
    "\n",
    "        # Store data in the global variable\n",
    "        extracted_data = {\n",
    "            \"extracted_text\": extracted_text,\n",
    "            \"language\": detected_language,\n",
    "            \"translated_text\": translated_text,\n",
    "            \"emotion\": detected_emotion,\n",
    "            \"confidence\": confidence_score\n",
    "        }\n",
    "\n",
    "        # Print output\n",
    "        print(\"\\n Extracted Information Stored:\")\n",
    "        print(f\"Extracted Text: {extracted_data['extracted_text']}\")\n",
    "        print(f\"Detected Language: {extracted_data['language']}\")\n",
    "        print(f\"Translated Text: {extracted_data['translated_text'] if extracted_data['language'].lower() != 'english' else 'N/A'}\")\n",
    "        print(f\"Detected Emotion: {extracted_data['emotion']}\")\n",
    "        print(f\"Confidence Score: {extracted_data['confidence']:.2f}\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\n Failed to process the text.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed image saved: processed_handwriting.jpg\n"
     ]
    }
   ],
   "source": [
    "# Preprocess handwriting image\n",
    "handwriting_path = './data/handwriting2.png'\n",
    "\n",
    "processed_image = preprocess_handwriting(handwriting_path)\n",
    "print(\"Preprocessed image saved:\", processed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Extracted Information Stored:\n",
      "Extracted Text: حكاية الصياد مع العفريت\n",
      "قالت بلغني أيها الملك السعيد انه كان رجل صياد وكان طاعن في السن وله زوجه وثلاثة أولاد وهو فقير الحال وكان من عادته انه يرمى شبكته كل يوم أربع مرات لا غير، ثم انه خرج يوماً من الأيام في وقت الظهر الى شاطئ البحر وحط شبكة ومضى إلى أن استقرت في الماء ثم جمع خيطها فوجدها ثقيلة فقيل في باله: يا ترى ماذا\n",
      "وجدت؟ ثم انه جرها وغطس في الماء وأخرج الشبكة ووجد فيها حماراً ميتاً فقال لا حول ولا قوة إلا بالله العلي العظيم. ثم قال إن هذا الرزق عجيب وانشد يقول:\n",
      "\n",
      "Detected Language: Arabic\n",
      "Translated Text: The text is in Arabic. Here is the translation to English:\n",
      "\n",
      "\"The Tale of the Fisherman and the Jinni\n",
      "\n",
      "It has been told, O happy king, that there was a fisherman who was advanced in age. He had a wife and three children and was poor. It was his habit to cast his net four times a day and no more. Then one day, at midday, he went out to the seashore, placed his net, and waited for it to settle in the water. When he pulled the string, he found it heavy and thought to himself: I wonder what I've caught? Then he pulled it, plunged into the water, and brought the net out to find a dead donkey inside. He said: There is no power and no strength except by God the Great. Then he said: This is a strange provision and recited:\n",
      "\n",
      "[The text ends before the verse.]\"\n",
      "Detected Emotion: Surprise\n",
      "Confidence Score: 0.85\n"
     ]
    }
   ],
   "source": [
    "final_result = process_handwritten_image(processed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Emotion and Incorporate Culture from Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Composition mapping based on detected emotion, grammar system\n",
    "COMPOSITION_MAPPING = {\n",
    "    \"Happiness\": \"Symmetrical & Balanced\",\n",
    "    \"Sadness\": \"Soft, Scattered & Faded\",\n",
    "    \"Anger\": \"Chaotic & Overlapping\",\n",
    "    \"Fear\": \"Dark & Enclosed\",\n",
    "    \"Surprise\": \"Expanding & Explosive\",\n",
    "    \"Disgust\": \"Distorted & Melting\",\n",
    "    \"Unknown\": \"Abstract Freeform\"\n",
    "}\n",
    "\n",
    "def assign_composition_style():\n",
    "    \"\"\"Assigns a composition layout based on the detected emotion.\"\"\"\n",
    "    global extracted_data\n",
    "\n",
    "    detected_emotion = extracted_data.get(\"emotion\", \"Unknown\")\n",
    "    composition_style = COMPOSITION_MAPPING.get(detected_emotion, COMPOSITION_MAPPING[\"Unknown\"])\n",
    "\n",
    "    extracted_data[\"composition_style\"] = composition_style\n",
    "\n",
    "    # print(f\"Assigned Composition Style for {detected_emotion}: {composition_style}\") # Debug print\n",
    "    return composition_style\n",
    "\n",
    "# Determine visual goal depending on the language detected\n",
    "def get_visual_goal():\n",
    "    \"\"\"Uses GPT-4 to generate cultural visual goal based on detected language.\"\"\"\n",
    "    \n",
    "    global extracted_data\n",
    "    \n",
    "    detected_language = extracted_data.get(\"language\", \"Unknown\")\n",
    "    \n",
    "    if detected_language == \"Unknown\":\n",
    "        print(\"Language not detected\")\n",
    "        return \"Unknown cultural element\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in global art and culture.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Provide only **two or three words** that represent a traditional artistic style, symbol, or pattern from the culture associated with the {detected_language} language. Do not give explanations. Just return the keywords.\"}\n",
    "            ],\n",
    "            max_tokens=100\n",
    "        )\n",
    "\n",
    "        visual_goal = response.choices[0].message.content.strip()\n",
    "        \n",
    "        return visual_goal\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"GPT API Error: {e}\")\n",
    "        return \"Unknown cultural element\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_cultural_prompt_info():\n",
    "    global extracted_data\n",
    "\n",
    "    detected_language = extracted_data.get(\"language\", \"Unknown\")\n",
    "\n",
    "    if detected_language == \"Unknown\":\n",
    "        print(\"Language not detected.\")\n",
    "        return {\n",
    "            \"culture\": detected_language,\n",
    "            \"art_form\": \"traditional art\",\n",
    "            \"motif\": \"cultural motif\",\n",
    "            \"script\": f\"{detected_language} script\"\n",
    "        }\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a cultural visual designer. Given a language, respond ONLY in raw JSON format with these 4 keys:\\n\"\n",
    "        \"'culture', 'art_form', 'motif', 'script'.\\n\"\n",
    "        \"Keep each value short (2–6 words). DO NOT return markdown or explanations.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"The language is: {detected_language}.\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            max_tokens=150\n",
    "        )\n",
    "\n",
    "        content = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Strip markdown/code block if GPT wraps it\n",
    "        if content.startswith(\"```\"):\n",
    "            content = content.strip(\"`\").strip()\n",
    "            # If there's still a language label, remove it\n",
    "            if content.startswith(\"json\"):\n",
    "                content = content[4:].strip()\n",
    "\n",
    "        # DEBUG PRINT\n",
    "        # print(\"GPT response (cleaned):\", content)\n",
    "\n",
    "        # Try parsing JSON\n",
    "        return json.loads(content)\n",
    "\n",
    "    except json.JSONDecodeError as je:\n",
    "        print(\"⚠️ Failed to parse GPT response as JSON.\")\n",
    "        print(\"Raw response:\\n\", content)\n",
    "        return {\n",
    "            \"culture\": detected_language,\n",
    "            \"art_form\": \"traditional art\",\n",
    "            \"motif\": \"cultural motif\",\n",
    "            \"script\": f\"{detected_language} script\"\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"GPT API Error: {e}\")\n",
    "        return {\n",
    "            \"culture\": detected_language,\n",
    "            \"art_form\": \"traditional art\",\n",
    "            \"motif\": \"cultural motif\",\n",
    "            \"script\": f\"{detected_language} script\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable Diffusion and Canvas for Drawing Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/diffusers/pipelines/pipeline_loading_utils.py:242: FutureWarning: You are loading the variant fp16 from stabilityai/stable-diffusion-2-1 via `revision='fp16'` even though you can load it via `variant=`fp16`. Loading model variants via `revision='fp16'` is deprecated and will be removed in diffusers v1. Please use `variant='fp16'` instead.\n",
      "  warnings.warn(\n",
      "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]An error occurred while trying to fetch /Users/michael/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1/snapshots/f7f33030acc57428be85fbec092c37a78231d75a/vae: Error no file named diffusion_pytorch_model.safetensors found in directory /Users/michael/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1/snapshots/f7f33030acc57428be85fbec092c37a78231d75a/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch /Users/michael/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1/snapshots/f7f33030acc57428be85fbec092c37a78231d75a/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /Users/michael/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1/snapshots/f7f33030acc57428be85fbec092c37a78231d75a/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...: 100%|██████████| 5/5 [00:01<00:00,  2.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# Stable Diffusion 2.1\n",
    "MODEL_ID = \"stabilityai/stable-diffusion-2-1\"\n",
    "\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    revision=\"fp16\"  # use float16 weights for efficiency\n",
    ")\n",
    "\n",
    "# Move to Apple Silicon GPU\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "pipeline.to(device)\n",
    "# Turn off NSFW filters due to overlapping stuff that may be detected as NSFW\n",
    "pipeline.safety_checker = None\n",
    "\n",
    "# Optimizations\n",
    "pipeline.enable_attention_slicing()\n",
    "pipeline.enable_vae_slicing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canvas \n",
    "CANVAS_WIDTH = 2560\n",
    "CANVAS_HEIGHT = 1440\n",
    "canvas = Image.new(\"RGBA\", (CANVAS_WIDTH, CANVAS_HEIGHT), (255, 255, 255, 0))\n",
    "\n",
    "def build_dynamic_prompt():\n",
    "    global extracted_data\n",
    "\n",
    "    composition_style = extracted_data.get(\"composition_style\", \"balanced composition\")\n",
    "    cultural_info = fetch_cultural_prompt_info()\n",
    "\n",
    "    culture = cultural_info[\"culture\"]\n",
    "    art_form = cultural_info[\"art_form\"]\n",
    "    motif = cultural_info[\"motif\"]\n",
    "    script = cultural_info[\"script\"]\n",
    "\n",
    "    prompt = (\n",
    "        f\"A high-resolution digital painting of a symbolic {culture} scene or object, \"\n",
    "        f\"inspired by traditional {art_form}, \"\n",
    "        f\"featuring {motif}, layered with {script}, \"\n",
    "        f\"with a {composition_style} composition. \"\n",
    "        f\"Rendered in 768x768, cinematic lighting, textured brushwork, Van Gogh style, and borderless.\"\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def generate_image():\n",
    "    \"\"\"Generates an image and assigns it to an agent for gradual drawing.\"\"\"\n",
    "    prompt = build_dynamic_prompt()\n",
    "    print(f\"Generating Image with Prompt: {prompt}\")\n",
    "\n",
    "    # Generate the image using Stable Diffusion 2.1 at native 768x768 resolution\n",
    "    image = pipeline(\n",
    "        prompt,\n",
    "        height=768,\n",
    "        width=768,\n",
    "        num_inference_steps=30,  # slightly higher for better quality\n",
    "        guidance_scale=8.0\n",
    "    ).images[0]\n",
    "\n",
    "    # Save image\n",
    "    image_path = \"generated_image.png\"\n",
    "    image.save(image_path)\n",
    "    print(f\"Image saved as: {image_path}\")\n",
    "\n",
    "    # Add image to agent system for live drawing\n",
    "    add_agent_for_image(image_path)\n",
    "\n",
    "    return image_path  # or transparent_image if you're removing background\n",
    "\n",
    "def add_agent_for_image(image_path):\n",
    "    global agents\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGBA\")\n",
    "    image = image.resize((600, 600))\n",
    "\n",
    "    x = random.randint(0, CANVAS_WIDTH - 600)\n",
    "    y = random.randint(0, CANVAS_HEIGHT - 600)\n",
    "\n",
    "    new_agent = Agent(image, x, y)\n",
    "    agents.append(new_agent)\n",
    "\n",
    "# Show live canvas    \n",
    "def show_live_canvas(canvas):\n",
    "    canvas_np = np.array(canvas.convert(\"RGB\"))[:, :, ::-1]  # PIL to OpenCV BGR\n",
    "    cv2.imshow(\"Live Canvas\", canvas_np)\n",
    "    key = cv2.waitKey(1)  # 1 ms delay to refresh window\n",
    "    if key == 27:  # Esc key to exit early\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def run_live_drawing_loop(steps=5000, delay=0.01):\n",
    "    for step in range(steps):\n",
    "        for agent in agents:\n",
    "            agent.update(canvas)\n",
    "\n",
    "        if step % 5 == 0:  # Show more frequently for smoother updates\n",
    "            if not show_live_canvas(canvas):\n",
    "                break  # Exit if Esc is pressed\n",
    "\n",
    "        time.sleep(delay)\n",
    "\n",
    "    cv2.destroyAllWindows()  # Close the window when done\n",
    "    canvas.save(\"final_collaborative_canvas.png\")\n",
    "    print(\"Saved as final_collaborative_canvas.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automate_processing():\n",
    "    \"\"\"Automates the full process based on user-uploaded image.\"\"\"\n",
    "    global extracted_data  # Ensure we store extracted information\n",
    "\n",
    "    # Ask the user to input an image file path\n",
    "    image_path = input(\"Enter the path to your handwriting image: \").strip()\n",
    "\n",
    "    # Extract handwritten text & detect language\n",
    "    extracted_text, detected_language = extract_text_from_handwriting(image_path)\n",
    "\n",
    "    if not extracted_text:\n",
    "        print(\"\\nFailed to extract text. Stopping process.\")\n",
    "        return None  # Stop if no text is found\n",
    "\n",
    "    # Translate text if needed\n",
    "    translated_text = translate_to_english(extracted_text) if detected_language.lower() != \"english\" else extracted_text\n",
    "\n",
    "    # Analyze emotion\n",
    "    text_analysis_data = analyze_emotion(translated_text)\n",
    "    detected_emotion = text_analysis_data.get(\"emotion\", \"Unknown\")\n",
    "    confidence_score = text_analysis_data.get(\"confidence\", 0.0)\n",
    "\n",
    "    # Store the extracted information in a global variable\n",
    "    extracted_data = {\n",
    "        \"extracted_text\": extracted_text,\n",
    "        \"language\": detected_language,\n",
    "        \"translated_text\": translated_text,\n",
    "        \"emotion\": detected_emotion,\n",
    "        \"confidence\": confidence_score,\n",
    "    }\n",
    "    \n",
    "    extracted_data[\"composition_style\"] = assign_composition_style()\n",
    "\n",
    "    extracted_data[\"visual_goal\"] = get_visual_goal()\n",
    "\n",
    "    # Print Final Summary\n",
    "    print(\"\\nAutomated Process Complete:\")\n",
    "    print(f\"Extracted Text: {extracted_data['extracted_text']}\")\n",
    "    print(f\"Detected Language: {extracted_data['language']}\")\n",
    "    print(f\"Translated Text: {extracted_data['translated_text'] if extracted_data['language'].lower() != 'english' else 'N/A'}\")\n",
    "    print(f\"Detected Emotion: {extracted_data['emotion']}\")\n",
    "    print(f\"Confidence Score: {extracted_data['confidence']:.2f}\")\n",
    "    print(f\"Assigned Composition Style: {extracted_data['composition_style']}\")\n",
    "    print(f\"Suggested Visual Goal: {extracted_data['visual_goal']}\")  \n",
    "    \n",
    "    generate_image()\n",
    "\n",
    "    # return extracted_data  # Return the full stored information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Automated Process Complete:\n",
      "Extracted Text: IAM HAPPY\n",
      "Detected Language: English\n",
      "Translated Text: N/A\n",
      "Detected Emotion: Happiness\n",
      "Confidence Score: 0.92\n",
      "Assigned Composition Style: Symmetrical & Balanced\n",
      "Suggested Visual Goal: Celtic knots, Morris wallpaper.\n",
      "Generating Image with Prompt: A high-resolution digital painting of a symbolic Western scene or object, inspired by traditional Theatre, featuring Tragedy and comedy, layered with Latin alphabet, with a Symmetrical & Balanced composition. Rendered in 768x768, cinematic lighting, textured brushwork, Van Gogh style, and borderless.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:36<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved as: generated_image.png\n"
     ]
    }
   ],
   "source": [
    "automate_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as final_collaborative_canvas.png\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Reset agent list\n",
    "agents = []\n",
    "\n",
    "# Add a test agent with any image\n",
    "add_agent_for_image(\"generated_image.png\")  # or any transparent image you have\n",
    "\n",
    "# Run the drawing loop\n",
    "run_live_drawing_loop(steps=5000, delay=0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
